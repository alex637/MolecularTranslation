{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet import some_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict()\n",
    "cfg['n_channels'] = 50\n",
    "cfg['input_channels'] = 1\n",
    "cfg['output_channels'] = 1 # 11\n",
    "cfg['dropout'] = 0.2\n",
    "cfg['fc_intermediate_len'] = 100 # ? 128\n",
    "cfg['x_size'] = 150 # ? such order of dims\n",
    "cfg['y_size'] = 300 # ? such order of dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pic(image_id, is_train=True):\n",
    "    _ = 'train' if is_train else 'test'\n",
    "    directory = 'original_data/{}/{}/{}/{}/'.format(_, image_id[0], image_id[1], image_id[2])\n",
    "    return torch.Tensor(imageio.imread(directory + image_id + '.png') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only Carbon counts\n",
    "class PretrainRegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, y_file, x_size=256, y_size=256):\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.y = torch.tensor(np.load(y_file)[:,1],\n",
    "                              dtype=torch.float32).unsqueeze(1) # only C\n",
    "        self.y.to(device)\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.resize = Resize(size=(x_size,y_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pic = load_pic(self.data['image_id'][index]).unsqueeze(0) # C=1 channel\n",
    "        pic = self.resize(pic) / 255.\n",
    "        y = self.y[index]\n",
    "        return pic, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(cnn, data_loader, loss_type='mse'):\n",
    "    if loss_type =='mse': \n",
    "        Loss = nn.MSELoss()\n",
    "    elif loss_type == 'mae':\n",
    "        Loss = nn.L1Loss(reduction='sum')\n",
    "    else:\n",
    "        return 'ERROR'\n",
    "\n",
    "    loss = 0.0\n",
    "    count = 0\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            predictions = cnn(x_batch)\n",
    "            loss += Loss(predictions, y_batch).item()\n",
    "            count += y_batch.shape[0] * y_batch.shape[1]\n",
    "    return loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, cnn, optimizer, criterion):\n",
    "    running_loss = 0.0\n",
    "    for x_batch, y_batch in tqdm(data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = cnn(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    #print('running_loss after epoch is', running_loss)\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_train, dataset_test, cnn, n_epochs, batch_size):\n",
    "    data_train_loader = torch.utils.data.DataLoader(dataset_train, batch_size,\n",
    "                                                   num_workers=10, persistent_workers=False,pin_memory=True)\n",
    "    data_test_loader  = torch.utils.data.DataLoader(dataset_test, batch_size, num_workers=2,pin_memory=False)\n",
    "\n",
    "    #criterion = nn.MSELoss()\n",
    "    #criterion = nn.L1Loss()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "    for i in range(n_epochs):\n",
    "        print('Starting epoch {}'.format(i))\n",
    "        cnn.train()\n",
    "        train_loss = train_epoch(data_train_loader, cnn, optimizer, criterion)\n",
    "        test_loss = calc_loss(cnn, data_test_loader, 'mae')\n",
    "        print('Train running loss (L1Smooth):\\t', train_loss)\n",
    "        print('Test loss (MAE):\\t\\t\\t', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_train = PretrainRegressionDataset('data/pretrain2/data_train.csv',\n",
    "                                          'data/pretrain2/atom_counts_train.npy',\n",
    "                                           x_size=cfg['x_size'], y_size=cfg['y_size'])\n",
    "dataset_test = PretrainRegressionDataset('data/pretrain2/data_test.csv',\n",
    "                                          'data/pretrain2/atom_counts_test.npy',\n",
    "                                          x_size=cfg['x_size'], y_size=cfg['y_size'])\n",
    "\"\"\"\n",
    "dataset_train = PretrainRegressionDataset('data/pretrain2/data_train_small.csv',\n",
    "                                          'data/pretrain2/atom_counts_train_small.npy',\n",
    "                                           x_size=cfg['x_size'], y_size=cfg['y_size'])\n",
    "dataset_test = PretrainRegressionDataset('data/pretrain2/data_test_small.csv',\n",
    "                                          'data/pretrain2/atom_counts_test_small.npy',\n",
    "                                          x_size=cfg['x_size'], y_size=cfg['y_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = some_model(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [01:49<00:00,  2.93it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train running loss (L1Smooth):\t 694.50158816576\n",
      "Test loss (MAE):\t\t\t 2.8286894969940186\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [01:49<00:00,  2.93it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train running loss (L1Smooth):\t 413.459808409214\n",
      "Test loss (MAE):\t\t\t 2.3172939224243163\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [01:52<00:00,  2.85it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train running loss (L1Smooth):\t 353.17281755805016\n",
      "Test loss (MAE):\t\t\t 1.2502723779678344\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [01:51<00:00,  2.86it/s]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train running loss (L1Smooth):\t 322.9752231836319\n",
      "Test loss (MAE):\t\t\t 2.100978073120117\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [01:51<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train running loss (L1Smooth):\t 302.7650328576565\n",
      "Test loss (MAE):\t\t\t 1.162437092781067\n",
      "CPU times: user 7min 33s, sys: 2min 30s, total: 10min 4s\n",
      "Wall time: 10min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(dataset_train, dataset_test, resnet, n_epochs=5, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.559694290161133 18.008541107177734 20.980161666870117 10.457900047302246 19.475666046142578 15.549481391906738\n",
      "13.0 19.0 21.0 11.0 21.0 16.0\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "resnet.eval()\n",
    "for index, (_, __) in enumerate(dataset_test):\n",
    "    if index > 5:\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        pred = resnet(_.unsqueeze(0).to(device)).item()\n",
    "    preds.append(pred)\n",
    "    trues.append(__.item())\n",
    "        \n",
    "print(*preds)\n",
    "print(*trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), 'data/resnet_v1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate results\n",
    "\n",
    "The chosen model seems to learn quite well with:\n",
    "- small dataset\n",
    "- SmoothL1Loss\n",
    "- Adam, lr=0.001\n",
    "\n",
    "## Training\n",
    "Now we add history saving parameter to the training procedure and start several epochs with a larger dataset, and smaller learning_rate for several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet.load_state_dict(torch.load('data/resnet_v1.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
