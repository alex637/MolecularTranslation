{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torchvision.transforms import Resize\n\nfrom tqdm import tqdm\nimport imageio\nimport gc\n\n#from sklearn.model_selection import train_test_split\n\n#from ResNet import ResNetEncoderPretrained, ResNet50_modified\n\nimport Levenshtein","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:26.849283Z","iopub.execute_input":"2021-06-13T17:48:26.849837Z","iopub.status.idle":"2021-06-13T17:48:27.275258Z","shell.execute_reply.started":"2021-06-13T17:48:26.849741Z","shell.execute_reply":"2021-06-13T17:48:27.274479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.277598Z","iopub.execute_input":"2021-06-13T17:48:27.278082Z","iopub.status.idle":"2021-06-13T17:48:27.315172Z","shell.execute_reply.started":"2021-06-13T17:48:27.27804Z","shell.execute_reply":"2021-06-13T17:48:27.314331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = dict()\n# some parameters\ncfg['n_channels'] = 50\ncfg['input_channels'] = 1\ncfg['output_channels'] = 6\ncfg['dropout'] = 0.2\n#cfg['fc_intermediate_len'] = 100 # ? 128\n\n# resizing images\ncfg['x_size'] = 150 \ncfg['y_size'] = 300 \n\n# for decoder\ncfg['attention_dim'] = 256\ncfg['embedding_dim'] = 512\ncfg['decoder_dim'] = 512\ncfg['max_len'] = 275\ncfg['num_lstm_layers'] = 2\n\n# for training\ncfg['train_dataset_file'] = '../input/data-from-laptop/train.csv'\ncfg['valid_dataset_file'] = '../input/data-from-laptop/valid.csv'","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.318457Z","iopub.execute_input":"2021-06-13T17:48:27.318959Z","iopub.status.idle":"2021-06-13T17:48:27.325677Z","shell.execute_reply.started":"2021-06-13T17:48:27.31893Z","shell.execute_reply":"2021-06-13T17:48:27.324472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Tokenizer(object):\n    \n    def __init__(self):\n        self.stoi = {}\n        self.itos = {}\n\n    def __len__(self):\n        return len(self.stoi)\n    \n    def fit_on_texts(self, texts):\n        vocab = set()\n        for text in texts:\n            vocab.update(text.split(' '))\n        vocab = sorted(vocab)\n        vocab.append('<sos>')\n        vocab.append('<eos>')\n        vocab.append('<pad>')\n        for i, s in enumerate(vocab):\n            self.stoi[s] = i\n        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n        \n    def text_to_sequence(self, text):\n        sequence = []\n        sequence.append(self.stoi['<sos>'])\n        for s in text.split(' '):\n            sequence.append(self.stoi[s])\n        sequence.append(self.stoi['<eos>'])\n        return sequence\n    \n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            sequence = self.text_to_sequence(text)\n            sequences.append(sequence)\n        return sequences\n\n    def sequence_to_text(self, sequence):\n        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n    \n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = self.sequence_to_text(sequence)\n            texts.append(text)\n        return texts\n    \n    def predict_caption(self, sequence):\n        caption = ''\n        for i in sequence:\n            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n                break\n            caption += self.itos[i]\n        return caption\n    \n    def predict_captions(self, sequences):\n        captions = []\n        for sequence in sequences:\n            caption = self.predict_caption(sequence)\n            captions.append(caption)\n        return captions\n\ntokenizer = torch.load('../input/data-from-laptop/tokenizer2.pth')\nprint(f\"tokenizer.stoi: {tokenizer.stoi}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.327642Z","iopub.execute_input":"2021-06-13T17:48:27.328007Z","iopub.status.idle":"2021-06-13T17:48:27.347875Z","shell.execute_reply.started":"2021-06-13T17:48:27.327972Z","shell.execute_reply":"2021-06-13T17:48:27.346754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_pic(image_id, is_train=True):\n    _ = 'train' if is_train else 'test'\n    directory = '../input/bms-molecular-translation/{}/{}/{}/{}/'.format(_, image_id[0], image_id[1], image_id[2])\n    return torch.Tensor(imageio.imread(directory + image_id + '.png') )","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.349114Z","iopub.execute_input":"2021-06-13T17:48:27.349481Z","iopub.status.idle":"2021-06-13T17:48:27.35529Z","shell.execute_reply.started":"2021-06-13T17:48:27.349446Z","shell.execute_reply":"2021-06-13T17:48:27.3543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, file, x_size=256, y_size=256, n_of_samples=None):\n        data = pd.read_csv(file)\n        self.n_of_samples = n_of_samples if n_of_samples else data.shape[0]\n        self.data = data[:self.n_of_samples].drop(columns=['InChI_1', 'InChI_length'])\n        \n        self.x_size = x_size\n        self.y_size = y_size\n        self.resize = Resize(size=(x_size,y_size))\n    \n    def __len__(self):\n        return self.n_of_samples\n    \n    def __getitem__(self, index):\n        pic = load_pic(self.data['image_id'][index]).unsqueeze(0) # C=1 channel\n        pic = self.resize(pic) / 255.\n        label = self.data['InChI_text'][index]\n        encoded_label = torch.LongTensor(tokenizer.text_to_sequence(label))\n        return pic, encoded_label, encoded_label.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.359333Z","iopub.execute_input":"2021-06-13T17:48:27.35991Z","iopub.status.idle":"2021-06-13T17:48:27.368137Z","shell.execute_reply.started":"2021-06-13T17:48:27.35987Z","shell.execute_reply":"2021-06-13T17:48:27.366933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Is used to form batches in DataLoader of Dataset __getitem__\n# Needed to create uniformly padded sequences for decoder\ndef CollateFunction(batch):\n    images = []\n    encoded_labels = []\n    label_lengths = []\n    for (image, encoded_label, label_length) in batch:\n        images.append(image)\n        encoded_labels.append(encoded_label)\n        label_lengths.append(label_length)\n    images = torch.stack(images)\n    encoded_labels = nn.utils.rnn.pad_sequence(encoded_labels, batch_first=True,\n                                               padding_value=tokenizer.stoi[\"<pad>\"])\n    #label_lengths = torch.stack(label_lengths).reshape(-1, 1)\n    label_lengths = torch.LongTensor(label_lengths).reshape(-1, 1)\n    return images, encoded_labels, label_lengths","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.369531Z","iopub.execute_input":"2021-06-13T17:48:27.370215Z","iopub.status.idle":"2021-06-13T17:48:27.37765Z","shell.execute_reply.started":"2021-06-13T17:48:27.37018Z","shell.execute_reply":"2021-06-13T17:48:27.376532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Encoder","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n        super(Bottleneck, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n        \n        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n        \n        self.i_downsample = i_downsample\n        self.stride = stride\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        identity = x.clone()\n        x = self.relu(self.batch_norm1(self.conv1(x)))\n        \n        x = self.relu(self.batch_norm2(self.conv2(x)))\n        \n        x = self.conv3(x)\n        x = self.batch_norm3(x)\n        \n        #downsample if needed\n        if self.i_downsample is not None:\n            identity = self.i_downsample(identity)\n        #add identity\n        x+=identity\n        x=self.relu(x)\n        \n        return x\n\n\nclass ResNetEncoder(nn.Module):\n    def __init__(self, ResBlock, layer_list, num_channels=3):\n        super(ResNetEncoder, self).__init__()\n        self.in_channels = 64\n        \n        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.batch_norm1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n        \n        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=128, stride=2) # 256\n        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=128, stride=2) # 512\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        #self.fc = nn.Linear(128*ResBlock.expansion, num_classes) # 512\n        \n    def forward(self, x):\n        x = self.relu(self.batch_norm1(self.conv1(x)))\n        x = self.max_pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        #x = x.reshape(x.shape[0], -1)\n        #x = self.fc(x)\n        \n        return x\n        \n    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n        ii_downsample = None\n        layers = []\n        \n        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n            ii_downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(planes*ResBlock.expansion)\n            )\n            \n        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n        self.in_channels = planes*ResBlock.expansion\n        \n        for i in range(blocks-1):\n            layers.append(ResBlock(self.in_channels, planes))\n            \n        return nn.Sequential(*layers)\n\ndef ResNet50_modified(channels=1):\n    return ResNetEncoder(Bottleneck, [3,4,6,3], channels)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.379336Z","iopub.execute_input":"2021-06-13T17:48:27.379596Z","iopub.status.idle":"2021-06-13T17:48:27.399842Z","shell.execute_reply.started":"2021-06-13T17:48:27.379572Z","shell.execute_reply":"2021-06-13T17:48:27.399117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decoder","metadata":{}},{"cell_type":"code","source":"####### RNN DECODER\n\n# attention module\nclass Attention(nn.Module):\n    '''\n    Attention network for calculate attention value\n    '''\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        '''\n        :param encoder_dim: input size of encoder network\n        :param decoder_dim: input size of decoder network\n        :param attention_dim: input size of attention network\n        '''\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n        self.relu        = nn.ReLU()\n        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n        return attention_weighted_encoding, alpha\n    \n    \n# custom LSTM cell\ndef LSTMCell(input_size, hidden_size, **kwargs):\n    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n    for name, param in m.named_parameters():\n        if 'weight' in name or 'bias' in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\n\n# decoder\nclass DecoderWithAttention(nn.Module):\n    '''\n    Decoder network with attention network used for training\n    '''\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers):\n        '''\n        :param attention_dim: input size of attention network\n        :param embed_dim: input size of embedding network\n        :param decoder_dim: input size of decoder network\n        :param vocab_size: total number of characters used in training\n        :param encoder_dim: input size of encoder network\n        :param num_layers: number of the LSTM layers\n        :param dropout: dropout rate\n        '''\n        super(DecoderWithAttention, self).__init__()\n        self.encoder_dim   = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim     = embed_dim\n        self.decoder_dim   = decoder_dim\n        self.vocab_size    = vocab_size\n        self.dropout       = dropout\n        self.num_layers    = num_layers\n        self.device        = device\n        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n        self.dropout       = nn.Dropout(p = self.dropout)\n        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid       = nn.Sigmoid()\n        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()                                      # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune = True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim = 1)\n        # debug\n        #print('mean_encoder_out.shape', mean_encoder_out.shape)\n        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n        # debug\n        #print('h[0].shape', h[0].shape)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        '''\n        :param encoder_out: output of encoder network\n        :param encoded_captions: transformed sequence from character to integer\n        :param caption_lengths: length of transformed sequence\n        '''\n        batch_size       = encoder_out.size(0)\n        encoder_dim      = encoder_out.size(-1)\n        vocab_size       = self.vocab_size\n        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels       = encoder_out.size(1)\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n        encoder_out      = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        \n        # embedding transformed sequence for vector\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        \n        # Initialize LSTM state, initialize cell_vector and hidden_vector\n        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        \n        # set decode length by caption length - 1 because of omitting start token\n        decode_lengths = (caption_lengths - 1).tolist()\n        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n        \n        # predict sequence\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                prev_h[-1][:batch_size_t])\n            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n\n            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n            \n            # debug\n            #print('input.shape =', input.shape)\n            #print('batch_size_t =', batch_size_t)\n            #print('prev_h[0][:batch_size_t].shape =', prev_h[0][:batch_size_t].shape)\n            \n            for i, rnn in enumerate(self.decode_step):\n                # recurrent cell\n                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n\n                # hidden state becomes the input to the next layer\n                input = self.dropout(h)\n\n                # save state for next time step\n                prev_h[i] = h\n                prev_c[i] = c\n                \n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :]      = alpha\n            \n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, tokenizer):\n        \n        # size variables\n        batch_size  = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size  = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels  = encoder_out.size(1)\n        \n        # embed start tocken for LSTM input\n        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n        embeddings    = self.embedding(start_tockens)\n        \n        # initialize hidden state and cell state of LSTM cell\n        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n        \n        # predict sequence\n        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n        for t in range(decode_lengths):\n            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n            awe        = gate * awe\n            \n            input = torch.cat([embeddings, awe], dim=1)\n \n            for j, rnn in enumerate(self.decode_step):\n                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n                input = self.dropout(at_h)\n                h[j]  = at_h\n                c[j]  = at_c\n            \n            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n            if end_condition.sum() == batch_size:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        \n        return predictions\n    \n    # beam search\n    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n        \n        h, c = hidden\n        #h, c = h.squeeze(0), c.squeeze(0)\n        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n        \n        embeddings = self.embedding(prev_tokens)\n        if embeddings.dim() == 3:\n            embeddings = embeddings.squeeze(1)\n            \n        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n        awe        = gate * awe\n        \n        input = torch.cat([embeddings, awe], dim = 1)\n        for j, rnn in enumerate(self.decode_step):\n            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n            input = self.dropout(at_h)\n            h[j]  = at_h\n            c[j]  = at_c\n\n        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n\n        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n        predicted_softmax = function(preds, dim = 1)\n        \n        return predicted_softmax, hidden, None","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.40152Z","iopub.execute_input":"2021-06-13T17:48:27.401878Z","iopub.status.idle":"2021-06-13T17:48:27.441238Z","shell.execute_reply.started":"2021-06-13T17:48:27.401845Z","shell.execute_reply":"2021-06-13T17:48:27.440438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train functions","metadata":{}},{"cell_type":"code","source":"def eval_performance(encoder, decoder, data_loader, batch_size, max_count=float('+inf')):\n    \n    encoder.eval()\n    decoder.eval()\n    count = 0\n    loss  = 0.\n    edit_dist = 0\n    \n    for (images, labels, label_lengths) in data_loader:\n        batch_size = images.shape[0]\n        \n        # make predictions (run forward)\n        images = images.to(device)\n        with torch.no_grad():\n            features = encoder(images)\n            # FIXME FOR STANDARD KAGGLE MODEL\n            features = features.reshape(images.shape[0], 1, 1, 512)\n            predictions = decoder.predict(features, cfg['max_len'], tokenizer)\n        \n        # transform predictions to text\n        predicted_sequences = torch.argmax(predictions.detach().cpu(), -1).numpy()\n        #     predictions: batch_size x cfg['max_len'] x len(tokenizer)\n        #     predicted_sequences = batch_size x cfg['max_len']\n        text_predictions = tokenizer.predict_captions(predicted_sequences)\n        #print('Text predictions:')\n        #print(text_predictions)\n        text_targets = tokenizer.predict_captions(labels.detach().cpu().numpy())\n        if count == 0:\n            print(text_predictions[0])\n        # calc metrics\n        for i in range(batch_size):\n            edit_dist += Levenshtein.distance(text_predictions[i], text_targets[i])\n        #loss += loss_function(predictions, labels)\n\n        count += batch_size\n        if count > max_count:\n            break\n    \n    edit_dist = edit_dist / count\n    loss = loss / count\n    # debug\n    loss = 11.0\n    return loss, edit_dist","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.443643Z","iopub.execute_input":"2021-06-13T17:48:27.444144Z","iopub.status.idle":"2021-06-13T17:48:27.455482Z","shell.execute_reply.started":"2021-06-13T17:48:27.444086Z","shell.execute_reply":"2021-06-13T17:48:27.454601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(data_loader, encoder, decoder, optimizer, criterion):\n    encoder.train()\n    decoder.train()\n    #for i_batch, (images, labels, label_lengths) in tqdm(enumerate(data_loader)): # labels already encoded\n    for (images, labels, label_lengths) in tqdm(data_loader):\n        \n        # 1. Move batch to GPU (if available)\n        images = images.to(device)\n        labels = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        \n        # 2. Zeroing gradients.\n        optimizer.zero_grad()\n        \n        # 3. Forward pass\n        features = encoder(images)\n        features = features.reshape(images.shape[0], 1, 1, 512)\n        \n        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n        targets = caps_sorted[:, 1:]\n        predictions = nn.utils.rnn.pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n        targets = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n        \n        # 4. Calculate loss.\n        loss = criterion(predictions, targets)\n        #if i_batch == 0: print('loss on 1st batch is', loss.item()) # debug\n        \n        # 5. Calculate gradients and perform optimization step.\n        loss.backward() # clip_grad_norm may be used after this\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.456805Z","iopub.execute_input":"2021-06-13T17:48:27.457241Z","iopub.status.idle":"2021-06-13T17:48:27.467937Z","shell.execute_reply.started":"2021-06-13T17:48:27.457206Z","shell.execute_reply":"2021-06-13T17:48:27.467269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(encoder, decoder, n_epochs, batch_size, history, finetune_encoder=False):\n    \n    # 1. Initialize and prepare data\n    dataset_train = TrainDataset(file=cfg['train_dataset_file'], \n                                 x_size=cfg['x_size'], y_size=cfg['y_size'], n_of_samples=200000)\n    \n    dataset_valid = TrainDataset(file=cfg['valid_dataset_file'],\n                                 x_size=cfg['x_size'], y_size=cfg['y_size'], n_of_samples=100)\n    \n    data_train_loader = torch.utils.data.DataLoader(dataset_train, batch_size,\n                                                   num_workers=4, persistent_workers=False, pin_memory=True,\n                                                   collate_fn=CollateFunction)\n    data_valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size,\n                                                    num_workers=2, persistent_workers=False, pin_memory=False,\n                                                    collate_fn=CollateFunction)\n    \n    # 2. Initialize loss and optimizer parameters\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi[\"<pad>\"]) # like in the link\n    \n    params_to_optimize = list(decoder.parameters())\n    if finetune_encoder:\n        params_to_optimize.extend(list(encoder.parameters()))\n    \n    optimizer = torch.optim.Adam(params_to_optimize, lr=0.0001)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n    \n    # 3. Move model to GPU (if available)\n    encoder.to(device)\n    decoder.to(device)\n    \n    # 4. Train for n_epochs, calculating loss/metrics and saving model parameters\n    for i in range(n_epochs):\n        print('Starting epoch {}'.format(i))\n        \n        train_epoch(data_train_loader, encoder, decoder, optimizer, criterion)\n        \n        # save model\n        torch.save(decoder.state_dict(), './decoder/decoder_v1_epoch_{}.model'.format(i))\n        if finetune_encoder:\n            torch.save(encoder.state_dict(), './decoder/encoder_v1_epoch_{}.model'.format(i))\n        \n        # print and save metrics\n        train_loss, train_metric = eval_performance(encoder, decoder, data_train_loader,\n                                                    batch_size, max_count=15)\n        valid_loss, valid_metric = eval_performance(encoder, decoder, data_valid_loader, batch_size)\n        print('Train loss:\\t{:f}, \\t validation loss:\\t{:f}'.format(train_loss, valid_loss))\n        print('Levenshtein distance: on train:\\t{},\\t on validation:\\t{}'.format(train_metric, valid_metric))\n        history[i,0] = train_loss\n        history[i,1] = valid_loss\n        history[i,2] = train_metric\n        history[i,3] = valid_metric\n        \n        # adapt learning rate\n        scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.469458Z","iopub.execute_input":"2021-06-13T17:48:27.469999Z","iopub.status.idle":"2021-06-13T17:48:27.484712Z","shell.execute_reply.started":"2021-06-13T17:48:27.469962Z","shell.execute_reply":"2021-06-13T17:48:27.48364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 5\nhistory = np.zeros((N_EPOCHS, 4))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.487822Z","iopub.execute_input":"2021-06-13T17:48:27.488065Z","iopub.status.idle":"2021-06-13T17:48:27.494533Z","shell.execute_reply.started":"2021-06-13T17:48:27.488037Z","shell.execute_reply":"2021-06-13T17:48:27.493738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = ResNet50_modified().to(device)\ndecoder = DecoderWithAttention(cfg['attention_dim'], embed_dim=cfg['embedding_dim'], \n                               decoder_dim=cfg['decoder_dim'], \n                               vocab_size=len(tokenizer), dropout=cfg['dropout'],\n                               device=device, encoder_dim=512, num_layers=cfg['num_lstm_layers']).to(device)\n# attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:27.496172Z","iopub.execute_input":"2021-06-13T17:48:27.496416Z","iopub.status.idle":"2021-06-13T17:48:29.409296Z","shell.execute_reply.started":"2021-06-13T17:48:27.496386Z","shell.execute_reply":"2021-06-13T17:48:29.408507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:29.410731Z","iopub.execute_input":"2021-06-13T17:48:29.411098Z","iopub.status.idle":"2021-06-13T17:48:29.487254Z","shell.execute_reply.started":"2021-06-13T17:48:29.411052Z","shell.execute_reply":"2021-06-13T17:48:29.486351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_model(encoder, decoder, N_EPOCHS, batch_size=100, history=history, finetune_encoder=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:48:29.488983Z","iopub.execute_input":"2021-06-13T17:48:29.489624Z","iopub.status.idle":"2021-06-13T20:21:45.354631Z","shell.execute_reply.started":"2021-06-13T17:48:29.489586Z","shell.execute_reply":"2021-06-13T20:21:45.353811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x__ = np.arange(N_EPOCHS)\nplt.plot(x__, history[:,2])\nplt.plot(x__, history[:,3])\nplt.grid(True)\nplt.legend(['train', 'validation'])\nplt.title('Levenshtein distance')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:24:03.277996Z","iopub.execute_input":"2021-06-13T20:24:03.278339Z","iopub.status.idle":"2021-06-13T20:24:03.452323Z","shell.execute_reply.started":"2021-06-13T20:24:03.278306Z","shell.execute_reply":"2021-06-13T20:24:03.451659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history[:,2]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:44:21.549542Z","iopub.execute_input":"2021-06-13T20:44:21.549886Z","iopub.status.idle":"2021-06-13T20:44:21.555982Z","shell.execute_reply.started":"2021-06-13T20:44:21.549854Z","shell.execute_reply":"2021-06-13T20:44:21.555111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history[:,3]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T20:44:34.181113Z","iopub.execute_input":"2021-06-13T20:44:34.181444Z","iopub.status.idle":"2021-06-13T20:44:34.187387Z","shell.execute_reply.started":"2021-06-13T20:44:34.181414Z","shell.execute_reply":"2021-06-13T20:44:34.18634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}