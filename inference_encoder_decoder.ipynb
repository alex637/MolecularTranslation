{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "## Auxiliary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Resize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import gc\n",
    "\n",
    "from ResNet import ResNetEncoderPretrained, ResNet50_modified\n",
    "\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict()\n",
    "# some parameters\n",
    "cfg['n_channels'] = 50\n",
    "cfg['input_channels'] = 1\n",
    "cfg['output_channels'] = 6\n",
    "cfg['dropout'] = 0.2\n",
    "#cfg['fc_intermediate_len'] = 100 # ? 128\n",
    "\n",
    "# resizing images\n",
    "cfg['x_size'] = 150 \n",
    "cfg['y_size'] = 300 \n",
    "\n",
    "# for decoder\n",
    "cfg['attention_dim'] = 256\n",
    "cfg['embedding_dim'] = 512\n",
    "cfg['decoder_dim'] = 512\n",
    "cfg['max_len'] = 275\n",
    "cfg['num_lstm_layers'] = 2\n",
    "\n",
    "# for training\n",
    "cfg['train_dataset_file'] = 'data/decoder/train.csv'\n",
    "cfg['valid_dataset_file'] = 'data/decoder/valid.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "    \n",
    "    def predict_captions_with_inchi(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = 'InChI=1S/' + self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "\n",
    "tokenizer = torch.load('preprocessing_output/tokenizer2.pth')\n",
    "print(f\"tokenizer.stoi: {tokenizer.stoi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pic(image_id, is_train=True):\n",
    "    _ = 'train' if is_train else 'test'\n",
    "    directory = 'original_data/{}/{}/{}/{}/'.format(_, image_id[0], image_id[1], image_id[2])\n",
    "    return torch.Tensor(imageio.imread(directory + image_id + '.png') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, x_size=256, y_size=256):\n",
    "        self.data = pd.read_csv(file)\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.resize = Resize(size=(x_size,y_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pic = load_pic(self.data['image_id'][index], is_train=False).unsqueeze(0) # C=1 channel\n",
    "        pic = self.resize(pic) / 255.\n",
    "        return pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        \n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        \n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        \n",
    "        #downsample if needed\n",
    "        if self.i_downsample is not None:\n",
    "            identity = self.i_downsample(identity)\n",
    "        #add identity\n",
    "        x+=identity\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, ResBlock, layer_list, num_channels=3):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
    "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
    "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=128, stride=2) # 256\n",
    "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=128, stride=2) # 512\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        #self.fc = nn.Linear(128*ResBlock.expansion, num_classes) # 512\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        #x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
    "        ii_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
    "            ii_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
    "            )\n",
    "            \n",
    "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
    "        self.in_channels = planes*ResBlock.expansion\n",
    "        \n",
    "        for i in range(blocks-1):\n",
    "            layers.append(ResBlock(self.in_channels, planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def ResNet50_modified(channels=1):\n",
    "    return ResNetEncoder(Bottleneck, [3,4,6,3], channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention network for calculate attention value\n",
    "    '''\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        '''\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        '''\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n",
    "        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "    \n",
    "# custom LSTM cell\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# decoder\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    '''\n",
    "    Decoder network with attention network used for training\n",
    "    '''\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers):\n",
    "        '''\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param num_layers: number of the LSTM layers\n",
    "        :param dropout: dropout rate\n",
    "        '''\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim   = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim     = embed_dim\n",
    "        self.decoder_dim   = decoder_dim\n",
    "        self.vocab_size    = vocab_size\n",
    "        self.dropout       = dropout\n",
    "        self.num_layers    = num_layers\n",
    "        self.device        = device\n",
    "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n",
    "        self.dropout       = nn.Dropout(p = self.dropout)\n",
    "        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n",
    "        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid       = nn.Sigmoid()\n",
    "        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()                                      # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune = True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim = 1)\n",
    "        # debug\n",
    "        #print('mean_encoder_out.shape', mean_encoder_out.shape)\n",
    "        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n",
    "        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n",
    "        # debug\n",
    "        #print('h[0].shape', h[0].shape)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        '''\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        '''\n",
    "        batch_size       = encoder_out.size(0)\n",
    "        encoder_dim      = encoder_out.size(-1)\n",
    "        vocab_size       = self.vocab_size\n",
    "        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels       = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        \n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        \n",
    "        # Initialize LSTM state, initialize cell_vector and hidden_vector\n",
    "        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n",
    "        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n",
    "        \n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                prev_h[-1][:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
    "            \n",
    "            # debug\n",
    "            #print('input.shape =', input.shape)\n",
    "            #print('batch_size_t =', batch_size_t)\n",
    "            #print('prev_h[0][:batch_size_t].shape =', prev_h[0][:batch_size_t].shape)\n",
    "            \n",
    "            for i, rnn in enumerate(self.decode_step):\n",
    "                # recurrent cell\n",
    "                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n",
    "\n",
    "                # hidden state becomes the input to the next layer\n",
    "                input = self.dropout(h)\n",
    "\n",
    "                # save state for next time step\n",
    "                prev_h[i] = h\n",
    "                prev_c[i] = c\n",
    "                \n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :]      = alpha\n",
    "            \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        \n",
    "        # size variables\n",
    "        batch_size  = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size  = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels  = encoder_out.size(1)\n",
    "        \n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n",
    "        embeddings    = self.embedding(start_tockens)\n",
    "        \n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n",
    "        \n",
    "        # predict sequence\n",
    "        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n",
    "        for t in range(decode_lengths):\n",
    "            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
    "            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
    "            awe        = gate * awe\n",
    "            \n",
    "            input = torch.cat([embeddings, awe], dim=1)\n",
    " \n",
    "            for j, rnn in enumerate(self.decode_step):\n",
    "                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
    "                input = self.dropout(at_h)\n",
    "                h[j]  = at_h\n",
    "                c[j]  = at_c\n",
    "            \n",
    "            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n",
    "            if end_condition.sum() == batch_size:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    # beam search\n",
    "    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n",
    "        \n",
    "        h, c = hidden\n",
    "        #h, c = h.squeeze(0), c.squeeze(0)\n",
    "        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n",
    "        \n",
    "        embeddings = self.embedding(prev_tokens)\n",
    "        if embeddings.dim() == 3:\n",
    "            embeddings = embeddings.squeeze(1)\n",
    "            \n",
    "        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
    "        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
    "        awe        = gate * awe\n",
    "        \n",
    "        input = torch.cat([embeddings, awe], dim = 1)\n",
    "        for j, rnn in enumerate(self.decode_step):\n",
    "            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
    "            input = self.dropout(at_h)\n",
    "            h[j]  = at_h\n",
    "            c[j]  = at_c\n",
    "\n",
    "        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
    "\n",
    "        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n",
    "        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n",
    "        predicted_softmax = function(preds, dim = 1)\n",
    "        \n",
    "        return predicted_softmax, hidden, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(dataset):\n",
    "    submission = pd.read_csv('original_data/sample_submission.csv')\n",
    "    data_loader = \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    index = 0\n",
    "    for images in tqdm(data_loader):\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        images = images.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = encoder(images)\n",
    "            features = features.reshape(images.shape[0], 1, 1, 512)\n",
    "            predictions = decoder.predict(features, cfg['max_len'], tokenizer)\n",
    "        \n",
    "        # transform predictions to text\n",
    "        predicted_sequences = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
    "\n",
    "        text_predictions = pd.Series(tokenizer.predict_captions_with_inchi(predicted_sequences) )\n",
    "        \n",
    "        submission.iat[index:(index+batch_size), 1] = text_predictions\n",
    "        \n",
    "        index += batch_size\n",
    "    submission.to_csv('submissions/enc_dec_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ResNet50_modified()\n",
    "encoder.load_state_dict(torch.load('data/decoder/best_encoder_v1.model'))\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "decoder = DecoderWithAttention(cfg['attention_dim'], embed_dim=cfg['embedding_dim'], \n",
    "                               decoder_dim=cfg['decoder_dim'], \n",
    "                               vocab_size=len(tokenizer), dropout=cfg['dropout'],\n",
    "                               device=device, encoder_dim=512, num_layers=cfg['num_lstm_layers'])\n",
    "decoder.load_state_dict(torch.load('data/decoder/best_decoder_v1.model'))\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TestDataset(x_size=cfg['x_size'], y_size=cfg['y_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_inference(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submissions/enc_dec_v1.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -k submissions/enc_dec_v1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
